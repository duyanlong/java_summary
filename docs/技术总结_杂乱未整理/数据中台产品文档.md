# 数据中台产品手册

## 整体介绍

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据中台底座作为数据中台产品的能力基础，包含离线存储、OLAP存储、调度中心、数据计算、数据传输五大系统，通过存储、计算、传输、调度能力的结合可实现数仓建设、数据分析的流水线建设；

**存储系统：**       
1、离线存储基于EMR Hadoop搭建，拥有分布式文件存储和分布式计算功能，HDFS
有高容错性的特点，并且设计用来部署在低廉的硬件上；而且它提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。Hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行，这里使用Spark替换MapReduce作为计算引擎，增加HQL的查询效率。            
2、OLAP存储使用ClickHouse实现大数据分析，ClickHouse支持实时的高并发系统、不依赖于Hadoop
 生态软件和基础、支持分布式机房的部署、安装和维护简单、查询速度快、可以支持SQL等特点。       

**数据计算系统：** 基于Spark+SQL开发的计算引擎，Spark基于内存计算，因此在效率上更加高效，继承了Spark
在分布式计算方面的优点；通过将数据计算逻辑抽象为计算单元，用户通过配置信息将计算单元编排为不同的计算逻辑快速实现数据需求；系统任务配置+动态参数解析，更好的支持了离线计算场景。

**数据传输系统：** 使用DataX作为离线数据同步工具，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS
等各种异构数据源之间高效的数据同步功能，通过服务化形式对外提供，实现了数据传输的服务化配置，为了支持更多场景进行了自定义插件开发。

**调度中心：** 基于Airflow开发的分布式调度系统，可通过编程方式灵活构建任务DAG；与IT架构中台结合更适合TME人；支持任务拓扑、失败重试、分布式调度、监控预警、可视化管理等功能。

### 整体架构

![](数据中台产品文档.assets\数据中台底座架构图.jpg)	        
<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中一期计算、存储、传输三层规划功能已完成并上线，投入使用，目前K歌投放、QA月报已基于数据中台底座建设上线；其他系统在接入中；

### 权限体系（现状）

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据权限安全主要涉及存储安全、操作安全、网络安全；因现阶段未实现页面化管理无法直接管控到"个人"，只管控到项目账号一级；       

#### 存储安全

- [X] HIVE存储安全：HIVE有DB privileges权限机制，可通过该机制对项目账号进行安全控制，可管控GRANT、REVOKE、CREATE、SELECT、DELETE、DROP
、ALTER等权限；
- [X] ClickHouse存储安全：Clickhouse可通过预配置项目账号权限为项目账号分配数据安全控制，基于RBAC模式（Role-based access control）；
- [ ] 技术业务管理拆分：但以上方式需通过人工操作来进行配置，流程比较繁琐，同时技术管理员无法进行控制（待解决）；
<br/>
- [ ] 长远规划：提供加密手段，可对指定库指定敏感字段进行加密存储，仅在应用的时候进行解密；
- [ ] 长远规划：提供脱敏手段，可对一些拉取到本数据平台需要脱敏的字段进行脱敏，实现数据不可逆向；
- [ ] 长远规划：提供销毁手段，对不再需要存储在数据平台中的数据进行彻底清理

#### 操作安全

- [X] 数据库操作：直接通过HIVE或Clickhouse方式访问可参考 存储安全 部分；要求：数据库操作应保留6个月或以上的操作记录，且安全人员可以获得该操作记录并进行定期审阅
- [X] 中台系统操作跟踪：基于数据计算和数据传输操作，操作记录会进行记录可进行查找记录；后面会接入日志中心
- [ ] 原生操作跟踪：原生的HIVE、Clickhouse架构日志比较分散，如果用户跳过中台而直接通过原生系统访问，日志不完善且比较分散；而且目前看来对我们是黑盒的(调研中)
- [ ] 技术业务管理拆分: 目前原生架构中系统管理员同时拥有业务管理权限，未在逻辑或物理上进行拆分（调研中）；数据库应进行分类，对于高级别数据通过信息安全隔离手段进行存储，高级别数据库非程序用账号使用分段管理的方式进行管理

#### 网络安全

- [X] 与外部数据源交互：业务系统有DB privileges权限机制，通过申请中台专用用户方式进行数据交互，避免账号混淆，可监控追踪；
- [X] 数据中台内部安全：数据中台服务器基于内网网络是相对安全的，同时分布式架构间有自己的rpc通信机制，可以保障中台内部服务于服务直接通信是安全的；
- [ ] 中台服务器访问安全：因EMR
服务器目前无法接入堡垒机进行管理，故需要通过堡垒机+跳板机形式进行控制连接来源、连接用户进行控制（进行中，通过堡垒机+跳板机访问），长远考虑会协调堡垒机满足所有类型的服务操作，且数据库操作通过堡垒机进行；

以上权限方案可实现基本安全保障，更高要求安全方案正在调研中；

### 关键特性

**易用性**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SDK+Server方式运行更易上手，目前支持的SDK为Python SDK，如需更多语言SDK支持可提需求；用户可通过SDK
一次注册作业多次使用，通过动态参数实现离线作业例行化，从而实现数据的流水线作业。

**安全性**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 服务器的作业接口使用Java去编写，由于Java是强类型语言，结构规范，大大降低接口出现安全性的问题，Java
处理多线程、缓存都有比较成熟的库，对性能和稳定性有一定提高；另外各系统都基于业界比较成熟安全的技术架构构建，系统更加安全稳定；

**兼容性**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 由于引进DataX这个数据传输的执行引擎，可以大大提高了各种数据类型进行传输，能最大限度的兼容各种数据类型的业务需求。 离线数据计算则引进Spark作为执行引擎，Spark基于内存，计算处理数据速度快，利用RDD结构提升了容错性能。

**灵活性**     
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 数据计算和传输使用json作为元数据，可编辑元数据快速接入作业；同时为平台化建设预留接口，接入更加灵活；
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 调度中心通过python dag方式构建调度任务，对于开发人员更加灵活；



### 联系我们
遇到相关问题可快速联系相关人员处理       
|系统名称|第一联系人|第一联系人电话|第二联系人|第二联系人电话|
|-|-|-|-|-|
|存储系统|xxxx(杜延龙)|13552932725|v_anaxxiong(熊安)|13689513650|
|数据计算|xxxx(杜延龙)|13552932725|v_anaxxiong(熊安)|13689513650|
|数据传输|xxxx(杜延龙)|13552932725|v_anaxxiong(熊安)|13689513650|
|调度中心|garykyang(杨凯风)|13823615583|xxxx(杜延龙)|13552932725|

## 数据存储

### 产品简介

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 数据存储分为离线存储和OLAP存储两大模块，可存储大量的离线数据，进行离线数仓分析，数仓数据经过过滤、计算后放在OLAP
存储，可以做大数据实时查询分析工作。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 离线数据存储我们使用的是Hadoop架构，其中hadoop核心是Hdfs、Yarn、Mapreduce，HDFS
是分布式文件存储系统组件，Yarn是计算资源管理组件，Mapreducer是计算架构，这里我们主要介绍HDFS组件，HDFS
有高容错性的特点，并且设计用来部署在低廉的硬件上；而且它提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 另外Hive也是我们离线存储用到的，其是在HDFS之上将数据文件映射为数据库表，将元数据存储于关系型数据库中，可提供SQL
查询功能，能将SQL语句转变成MapReduce、Spark、Tez等计算任务对数据进行分析计算；    

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OLAP存储使用ClickHouse，ClickHouse支持实时的高并发系统、不依赖于Hadoop
生态软件和基础、支持分布式机房的部署、安装和维护简单、查询速度快、可以支持SQL等特点。

**参考文档（Hive、Hadoop、Clickhouse）**
hadoop使用文档参照：https://hadoop.apache.org/docs/r3.1.4/
hive使用文档参照：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML
Clickhouse使用文档参照：https://clickhouse.tech/docs/zh/sql-reference/statements/misc/#drop-table

### 产品优势

#### 灵活

* 存储与计算分离，基于腾讯EMR，可快速弹性伸缩存储与计算资源，通过管理页面可以自行管理，使用更加灵活；
#### 可靠

* Master 节点容灾设计，备节点秒级拉起，保障大数据服务可用性。
* 完善的监控体系建设，您可以通过短信渠道秒级感知集群组件及任务的运行异常状况。
* 支持将 Hive 元数据存放于 MetaDB，元数据可靠性达99.9996%。
* 支持分析存放于 COS 的高存储耐久性的 PB 级数据。
* 集群默认开启回收站功能，提供误删除设备的找回机制。

#### 安全
* 可通过便捷的 VPC 网络安全隔离手段规划托管 Hadoop 集群网络策略，支持网络 ACL 和安全组，可从子网和节点维度筛选流量，全方位满足网络安全需求。
* 腾讯云品质的安全加固服务为 EMR 集群提供一体化的安全服务，涵盖网络防护、入侵检测、漏洞防护等。
* 提供集群级别的 Kerberos 认证，保障集群访问安全。

#### 易用
* 近千项集群级、组件级监控指标，搭配监控概览页面，提供丰富且清晰易用的监控系统。
* 灵活支撑云端多机型集群，实现对异构配置集群在扩容、配置下发等场景下的轻松应对，以更优硬件配置应对业务分析挑战。
* 使用习惯贴近服务器文件系统，上手快

#### 节约成本
* 通过 EMR 服务，可以按业务曲线随心伸缩托管 Hadoop 集群，缩减高昂的硬件成本。
* 丰富的运维工具支持，大幅提升运维工作效率，让工程师更专注于业务本身的商业价值，摆脱重复搭建监控、安全、运维工具等基础设施。
* 支持温冷数据的对象存储 COS/CHDFS 存储，成本有效降低28% - 50%。
* 结合统一 Hive 元数据库以及统一对象存储，实现跨集群的同数据集分析架构，集群按需创建或销毁，节省集群柔性成本。

### 实战演练
#### 客户端访问
* hadoop
```shell
[hadoop@10 ~]$ hadoop fs -ls /user/hdp-test/xxxx_test/20210101
Found 2 items
-rw-r--r--   3 hadoop supergroup      28088 2021-08-06 16:45 /user/hdp-test/xxxx_test/20210101/test_order__d3f1f2b7_590b_450b_9c49_ed3fe9ea9d7f
-rw-r--r--   3 root   supergroup      29488 2021-08-25 20:15 /user/hdp-test/xxxx_test/20210101/test_order__fdc2647b_6c8f_4908_a840_5bc764777643

[hadoop@10 ~]$ hadoop fs -text /user/hdp-test/xxxx_test/20210101/* |tail -4
450,1001,0382491dfec4b1d8b6b131d4dcb35d3b,cheny,fe566aa9b3aac90347b3e85c93df6d9d,1001
451,1001,0382491dfec4b1d8b6b131d4dcb35d3b,aa.json,d62992e5fb615a5e37b15ea1c2449117,1001
452,1001,0382491dfec4b1d8b6b131d4dcb35d3b,lanson,947c36a821370c0a2cd76c5323ff7e2f,1001
453,1001,0382491dfec4b1d8b6b131d4dcb35d3b,cheny,3afa9319273af0314567718c787b12a0,1001
```
* hive   
```shell
hadoop@10 ~]$ hive
hive> show tables;
OK
test01
test03
test04
test05
```

* OLAP存储      
- 找 杜延龙 申请登录权限，测试服务器：xx.xx.xxx.xx       
- 直接使用clickhouse-client -u user --password 'password'命令进入交互窗口执行sql操作        

#### http访问
* hadoop        
测试环境：http://xx.xx.xxx.xx:4008/explorer.html#/      
* Clickhouse      
```
curl xx.xx.xxx.xx:8123 -d 'select * from test.BIRTH_TEST_COUNT limit 5'
# 查询结果
0	河北	唐山	908	    0
0	河北	邯郸	2897	0
0	河北	廊坊	2040	0
0	河北	石家庄	3598    0
0	河北	沧州	2010	0
```

### 常见问题
* HDFS无权限、Hive表有权限不一致，导致写入数据失败      
**解决方法：** 统一HDFS和Hive权限
* Clickhouse写入数据时空值异常
**解决办法：** Clickhouse中和关系数据库中字段值允许为空配置不同，通过Nullable(Integer|String)  方式配置允许为空
* Clickhouse为了提高效率尽量避免删除、修改数据
**解决办法：** 可通过条件筛选数据，尽量使用增量插入

## 数据计算

### 产品简介

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 数据计算系统基于Spark分布式计算架构构建，通过server+sdk方式调用，使用灵活，计算快速；可满足离线场景数据计算分析需求。

系统中用户需要关心的信息主要包括如下部分： 

|名称|适应人群|用途描述|备注|
|---|---|---|---|
| 集群 | 中台维护人员 | 用于管理数仓库表信息和计算、存储资源的范围区分；由底层hadoop、hive等基础架构抽象而来 | 跨集群提交作业依赖于此项，由于当前没有平台页面管理，暂时由安装时中台配置；后期平台化后直接页面操作 |
| 项目 | 中台维护人员 | 项目账号用来实现多租户隔离的逻辑标识，默认直接和sdk绑定 | 在sdk安装时会由中台维护人员配置，如需增加新项目找中台维护人员申请，后期平台直接操作 |
| 作业 | 数仓开发人员 | 用来开发具体业务逻辑计算作业，目前支持简单sql和复杂配置两种作业 | 目前复杂配置支持：跨存储读写、多源数据合并、数据多源输出、复杂sql计算等 |
| 作业逻辑单元 | 数仓开发人员 | 计算作业中读、算、写等不同子流程<br/>简单sql作业：只有sql计算单个逻辑单元 <br/> 复杂配置作业：支持跨存储读写、多源数据合并、数据多源输出等不同逻辑单元，上面提到的mysql、clickhouse写即为逻辑单元 | 目前支持：<br/>mysql读<br/>hive读<br/> sql处理 <br/> mysql写<br/>clickhouse写<br/>hive写 |
| 日期规则 | 数仓开发人员 | 执行离线批作业的日期替换规则，针对制定逻辑需要动态时间作为参数进行零落计算的需求，可以通过该规则进行替换 | 目前支持日期直接替换<br/> 多日期参数计算替换 <br/> 日期范围等，具体语法参照<br/>《日期替换引擎》文档 |

#### 部署目录

```text
calc
├── bin
│   ├── config.cfg  sdk配置
│   ├── conf_job.py  json配置任务并执行sdk
│   ├── sql_job.py   sql配置任务并执行sdk
│   ├── register_conf.py  注册json sdk
│   ├── register_sql.py 注册sql sdk
│   ├── run_job.py 执行作业sdk
│   ├── search_job.py  查找作业sdk
│   └── update_job.py  更新作业配置sdk
├── calc-spark.jar  数据计算程序包
├── conf 服务配置目录
│   ├── application.properties 服务配置
│   ├── clusters  集群配置目录
│   │   └── dev_conf  开发集群配置目录
├── datax  数据传输程序目录
│   ├── bin 数据传输原生sdk目录
│   │   ├── datax.py
│   │   ├── dxprof.py
│   │   └── perftrace.py
│   ├── conf 数据传输客户端配置目录
│   │   ├── core.json
│   │   ├── logback.xml
│   │   └── server.properties
│   ├── lib 数据传输程序包目录
│   │   ├── *.jar
│   ├── log 日志目录
│   ├── log_perf 日志目录
│   ├── plugin  数据传输插件目录
│   │   ├── reader  读插件目录
│   │   │   ├── hdfsreader hdfs读插件
│   │   │   │   ├── libs
│   │   │   │   │   ├── apache-curator-2.6.0.pom
│   │   │   │   │   └── apache-curator-2.7.1.pom
│   │   │   │   ├── plugin_job_template.json
│   │   │   │   └── plugin.json
│   │   │   ├── httpreader  http接口读插件
│   │   │   │   ├── libs
│   │   │   │   ├── plugin_job_template.json
│   │   │   │   └── plugin.json
│   │   └── writer  写插件目录
│   │       ├── clickhousewriter clickhouse写插件
│   │       │   ├── libs
│   │       │   ├── plugin_job_template.json
│   │       │   └── plugin.json
│   │       ├── elasticsearchwriter  ES写插件
│   │       │   ├── libs
│   │       │   └── plugin.json
│   │       ├── ftpwriter   FTP写插件
│   │       │   ├── libs
│   │       │   ├── plugin_job_template.json
│   │       │   └── plugin.json
│   │   └── Readme.md  项目描述文件
├── libs 中台底座服务程序包
│   └── job-server.jar
├── logs 中台底座日志
│   └── server.log
└── sbin 中台底座服务管理脚本
    ├── start-server.sh
    └── stop-server.sh
```

#### SDK说明
- search_job.py   
  根据作业关键信息查找作业，用户作业找回等操作      
  命令提示：

```shell
Usage: python3 search_job.py [-i 'create user' -u 'update user' -n 'job name' -j 'job info']
Usage: python3 search_job.py [--insertUser 'create user' --updateUser 'update user' --jobName 'job name' --jobInfo 'job info']
```

- register_conf.py  
  传入作业配置文件路径和作业名称（可选），进行作业注册返回作业id   
  命令提示：

```shell
                    Usage: python register_conf.py 配置文件路径 [作业名称]
                    Usage: python register_conf.py [-f 配置文件路径] [-n 作业名称] [-e 配置文件压缩加密字符串] [-t 租户id] [-p 作业运行配置] [-g 执行引擎]
                    Usage: python register_conf.py [--filepath=配置文件路径] [--jobname=作业名称] [--encodedhex=配置文件压缩加密字符串] [--tenant=租户id] [--params=作业运行配置] [--engine=执行引擎]                    
                    
                参数描述    --filepath :配置文件路径 如：./example/example.json
                           --jobname :作业名称 如：job_001
                           --encodedhex :配置文件压缩加密字符串
                           --tenant ：租户id 
                           --params ：运行配置参数，如：{"param":["-Xms1G","-Xmx2G"],"config":{"partition":"20210101"}}
                           --engine : 执行引擎： 0、spark(默认) 1、hive 2、flink 3、flume 4、datax
```

- register_sql.py  
  传入sql和作业名（可选），根据sql注册为作业        
  命令提示：  

```shell
Usage: python3 register_sql.py sql [job_name]
```

- run_job.py    
  输入作业id和作业运行日期，提交运行作业，根据作业id和运行日期监控作业状态，单作业阻塞执行   
  命令提示：   

```shell
        Usage: python run_job.py jobId [rundate]
        Usage: python run_job.py -j 'job id' [-d 'run thedate' -x Flase -c '{"channel":"123", "version":"v1001"}']
        Usage: python update_job.py --jobId 'job id' [--thedate 'run thedate' --isdebug False --runconf '{"channel":"123", "version":"v1001"}']
        参数描述：
            -h,--help: 帮助
            -j,--jobId: 作业id
            -d,--thedate: 作业运行日期
            -x,--isdebug: 默认false，作业运行模式，调试 or 正式运行
            -c,--runconf: 作业运行配置 {"channel":"123", "version":"v1001"}
```

- update_job.py            
  根据作业id，更新作业信息       
  命令提示： 

```shell
Usage: python3 update_job.py -j 'job id' [-n 'job name' -f 'job config file' -m 'job mark']
Usage: python3 update_job.py --jobId 'job id' [--jobName 'job name' --filePath 'job config file' --mark 'job mark']
```

- 注册执行SQL一体化脚本      
  将注册SQL、检查是否存在、执行SQL等流程集成在一起，简化流程，优化用户使用流程

  命令提示：

```
Usage: python3 sql_job.py sql [job_name]
```

### 产品优势

#### 运行速度快

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10
倍以上，如果数据从内存中读取，速度可以高达100多倍。

#### 易用性好

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 以Json+Sql方式配置程序逻辑，技术门槛低，更易上手。

#### 通用性强
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 生态圈丰富，更加通用；Spark生态圈即BDAS（伯克利数据分析栈）包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX等组件，这些组件分别处理Spark Core提供内存计算框架、SparkStreaming的实时处理应用、Spark SQL的即席查询、MLlib或MLbase的机器学习和GraphX的图处理。

#### 适应性强
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 针对新需求可快速支持；Spark能够读取HDFS、Cassandra、HBase、S3和Techyon为持久层读写原生数据，能够以Mesos、YARN和自身携带的Standalone作为资源管理器调度job，来完成Spark应用程序的计算。

#### 新上功能
* ClickHouse条件覆盖数据功能开发
* 动态参数
* 日期参数规划计算 （参考日期替换引擎） 
* 数据计算服务化
* 数据计算clickhouse读写、mysql读写、Spark SQL功能上线

### 实战演练
#### 简单SQL计算样例

1、创建作业获取作业id:

```shell
python3 /data/calc/bin/register_sql.py 'create table test06 as select * from test03 where pday=${date_add(Ymd,1)}'
66
```

2、执行作业：

```shell
python3 /data/calc/bin/run_job.py 66 '20210602'
作业提交成功
作业状态为: 0
作业状态为: 2
作业状态为: 2
作业状态为: 3
True
```

3、验证结果：

```shell
hive -e "select * from test06";

河南	安阳	708	    20210602
河南	信阳	908	    20210602
河南	洛阳	3078	20210602
河南	郑州	3597	20210602
```

#### 复杂任务样例

1、预编写作业json /data/calc/example/muti_source.json

```json
{
    "job_type":"sql",
    "job_name":"test001",
    "params":{},
    "hive_meta_uri":"thrift://xx.xx.xxx.xx:7004",
    "ruotes":[
        {
            "before":"1",
            "after":"2"
        },
        {
            "before":"2",
            "after":"3"
        }
    ],
    "tasks":{
        "1":{
            "url":"jdbc:mysql://xx.xx.xxx.xx:3306/test",
            "user":"root",
            "password":"Kq0FYUSc42",
            "tempView":"mysql_tbl01",
            "sql":" select PROVINCE,CITY,BIRTH FROM  BIRTH_TEST ",
            "name":"mysql_reader",
            "strategy":"mysql.reader"
        },
        "2":{
            "tempView":"hive_tbl01",
            "sql":"select * from mysql_tbl01 union all select province,city,birth from test01 where pday=${date(Ymd)}",
            "name":"hive.sql",
            "strategy":"hive.sql"
        },
        "3":{
            "url":"jdbc:clickhouse://xx.xx.xxx.xx:8123/test",
            "tempView":"mysql_tbl01",
            "table":" CALC_TEST",
            "columns":[
                "PROVINCE",
                "CITY",
                "BIRTH"
            ],
            "name":"clickhouse.writer",
            "strategy":"clickhouse.writer"
        }
    }
}
```

2、注册作业：

```shell
python3 /data/calc/bin/register_conf.py '/data/calc/example/muti_source.json'
67
```

3、清空目标数据：

```shell
ssh hdp-test@xx.xx.xxx.xx -p36000 
# 输入密码

[hdp-test@10 ~]$ clickhouse-client -u username --password 'password'
# 测试前情况clickhouse结果表，避免和别人的冲突
xx.xx.xxx.xx :) ALTER TABLE CALC_TEST DELETE WHERE 1=1

ALTER TABLE CALC_TEST
    DELETE WHERE 1 = 1
```

4、执行作业：

```shell
python3 /data/calc/bin/run_job.py 67 '20210908'
作业提交成功
作业状态为: 0
作业状态为: 2
作业状态为: 4
```

5、验证结果：

```shell
xx.xx.xxx.xx :) SELECT * FROM test.CALC_TEST 

0	广西	桂林	 908
0	广西	玉林	 2897
0	广西	河池	 2040
0	广西	柳州	 3598
0	广西	钦州	 2010
0	广西	北海	 708
0	贵州	毕节	 1002
0	贵州	贵阳	 2008
0	贵州	安顺	 1023
0	贵州	铜仁	 1832
0	贵州	遵义	 1203
0	贵州	六盘水	 1652
0	河北	唐山	 908	
0	河北	邯郸	 2897
0	河北	廊坊	 2040
0	河北	石家庄	3598
0	河北	保定	 2010
```


### 常见问题
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 保留章节，后续补充用；



## 数据传输
### 产品简介

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 数据传输系统使用DataX作为离线数据同步工具，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS 等各种异构数据源之间高效的数据同步功能。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下列表，详情请点击：[DataX数据源参考指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)

| 类型               | 数据源                          | Reader(读) | Writer(写) | 文档                                                         |
| ------------------ | ------------------------------- | ---------- | ---------- | ------------------------------------------------------------ |
| RDBMS 关系型数据库 | MySQL                           | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md) |
|                    | Oracle                          | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md) |
|                    | SQLServer                       | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md) |
|                    | PostgreSQL                      | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md) |
|                    | DRDS                            | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md) |
|                    | 通用RDBMS(支持所有关系型数据库) | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/rdbmsreader/doc/rdbmsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/rdbmswriter/doc/rdbmswriter.md) |
| 阿里云数仓数据存储 | ODPS                            | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/odpswriter/doc/odpswriter.md) |
|                    | ADS                             |            | √          | [写](https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md) |
|                    | OSS                             | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md) |
|                    | OCS                             | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/ocsreader/doc/ocsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md) |
| NoSQL数据存储      | OTS                             | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md) |
|                    | Hbase0.94                       | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md) |
|                    | Hbase1.1                        | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md) |
|                    | Phoenix4.x                      | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hbase11xsqlreader/doc/hbase11xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xsqlwriter/doc/hbase11xsqlwriter.md) |
|                    | Phoenix5.x                      | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hbase20xsqlreader/doc/hbase20xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase20xsqlwriter/doc/hbase20xsqlwriter.md) |
|                    | MongoDB                         | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/mongodbreader/doc/mongodbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mongodbwriter/doc/mongodbwriter.md) |
|                    | Hive                            | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md) |
|                    | Cassandra                       | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/cassandrareader/doc/cassandrareader.md) 、[写](https://github.com/alibaba/DataX/blob/master/cassandrawriter/doc/cassandrawriter.md) |
| 无结构化数据存储   | TxtFile                         | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md) |
|                    | FTP                             | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md) |
|                    | HDFS                            | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md) |
|                    | Elasticsearch                   |            | √          | [写](https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md) |
| 时间序列数据库     | OpenTSDB                        | √          |            | [读](https://github.com/alibaba/DataX/blob/master/opentsdbreader/doc/opentsdbreader.md) |
|                    | TSDB                            | √          | √          | [读](https://github.com/alibaba/DataX/blob/master/tsdbreader/doc/tsdbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/tsdbwriter/doc/tsdbhttpwriter.md) |

### 产品优势
#### 成熟稳定
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 基于阿里开源DataX实现，业界应用广泛，比较稳定；
#### 易扩展
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 采用核心架构+插件模式，针对新数据源可通过插件开发方式快速构建支持
#### 弱依赖
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 外部依赖较少，可快速迁移，无需部署复杂的依赖架构

#### 新上功能
* 动态参数
* 日期参数规划计算 （参考日期替换引擎）
* 数据传输服务化
* 数据传输Http Reader、Hive Writer插件支持

### 实战演练

1、源数据准备：  

![](数据中台产品文档.assets\sample_dbgui.jpg)

2、预准备作业配置：

```shell
// 作业配置
cat /data/calc/datax**/sample/xxxx_test.json 
```
```json
{
  "job": {
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "column": [
              "*"
            ],
            "connection": [
              {
                "jdbcUrl": [
                  "jdbc:mysql://10.130.xx.xxx:3306/calc_db?useUnicode=true&characterEncoding=utf-8&useSSL=false&rewriteBatchedStatements=true"
                ],
                "querySql": [
                	"select id,tenant_id,res_id,job_name,chk_sum,cluster_id from PGM_BATCH_JOB_INFO"
		]
              }
            ],
            "password": "*****",
            "username": "*****"
          }
        },
        "writer": {
          "name": "hdfswriter",
          "parameter": {
            "defaultFS": "hdfs://10.130.xx.xx:4007",
            "fileType": "text",
            "path": "/user/hdp-test/xxxx_test/${partition}",
            "fileName": "test_order",
            "column": [
              {
                "name": "id",
                "type": "string"
              },
              {
                "name": "tenant_id",
                "type": "string"
              },
              {
                "name": "res_id",
                "type": "string"
              },
              {
                "name": "job_name",
                "type": "string"
              },
              {
                "name": "chk_sum",
                "type": "string"
              },
              {
                "name": "cluster_id",
                "type": "string"
              }
            ],
            "writeMode": "append",
            "fieldDelimiter": ","
          }
        }
      }
    ],
    "setting": {
      "speed": {
        "channel": 2
      }
    }
  }
}
```

3、注册作业：      
```shell 
python3 /data/calc/bin/register_conf.py -h
                    Usage: python register_conf.py 配置文件路径 [作业名称]
                    Usage: python register_conf.py [-f 配置文件路径] [-n 作业名称] [-e 配置文件压缩加密字符串] [-t 租户id] [-p 作业运行配置] [-g 执行引擎]
                    Usage: python register_conf.py [--filepath=配置文件路径] [--jobname=作业名称] [--encodedhex=配置文件压缩加密字符串] [--tenant=租户id] [--params=作业运行配置] [--engine=执行引擎]                    
                    
                参数描述    --filepath :配置文件路径 如：./example/example.json
                           --jobname :作业名称 如：job_001
                           --encodedhex :配置文件压缩加密字符串
                           --tenant ：租户id 
                           --params ：运行配置参数，如：{"param":["-Xms1G","-Xmx2G"],"config":{"partition":"20210101"}}
                           --engine : 执行引擎： 0、spark(默认) 1、hive 2、flink 3、flume 4、datax

python3 /data/calc/bin/register_conf.py -f /data/calc/datax/sample/xxxx_test.json -g 4
注册作业为： 19
```

4、执行作业：     

```shell
python3 /data/calc/bin/run_job.py -h
        Usage: python run_job.py jobId [rundate]
        Usage: python run_job.py -j 'job id' [-d 'run thedate' -x Flase -c '{"channel":"123", "version":"v1001"}']
        Usage: python update_job.py --jobId 'job id' [--thedate 'run thedate' --isdebug False --runconf '{"channel":"123", "version":"v1001"}']
        参数描述：
            -h,--help: 帮助
            -j,--jobId: 作业id
            -d,--thedate: 作业运行日期
            -x,--isdebug: 默认false，作业运行模式，调试 or 正式运行
            -c,--runconf: 作业运行配置 {"channel":"123", "version":"v1001"}

python3 /data/calc/bin/run_job.py -j 19 -c '{"partition":"20210808"}'     
```

5、验证数据：       
只贴出了部分数据    

![](数据中台产品文档.assets\sample_hdfsdata.jpg)

### 常见问题
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 保留章节，后续补充用；

## 调度中心

### 产品简介

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 调度中心是基于Airflow搭建的，Airflow是一个用于编排复杂计算工作流和数据处理流水线的开源工具，可灵活且快速扩展的工作流自动化调度系统；通常可以解决一些复杂超长 Cron脚本任务或者大数据的批量处理任务，以下是Airflow组成部分：

**Web server：**提供图形界面，可以监控数据流水线，检查任务是否正确执行，新建与外部系统的连接等等。
**Scheduler：**任务调度器。
**Metadata Database：**储存元数据，比如mysql。
**Executor：**定义任务的执行。
**Worker：**处理任务的进程，接收和处理任务

![](数据中台产品文档.assets\image-20210909152318609.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Airflow工作流的设计是基于有向无环图 (Directed Acyclical Graphs, DAG) ，用于设置任务依赖关系和时间调度。

![image-20210909152346580](数据中台产品文档.assets\image-20210909152346580.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一个DAG代表一个工作流，一个 DAG 中包含多个 Task，多个 Task 组成一个有向无环图。
**官网：** https://airflow.apache.org
#### 目录结构
```
airflow
│   README.md              文档
│   airflow.cfg-example    airflow.cfg demo 
│   
│
└───dags       核心目录，存放各项目的dag
│   └───dashboard    dashboard业务
│       └───dashboard_dag    dashboard项目的dag
│           │   xx.python
│           │   ...
│
│   └───data_center    数据中台业务
│       └───haq        质量项目的dag
│            │   xx.python
│            │   ...
│
│   └───sdk    SDK目录
│        └───middle_platform     数据中台SDK
│        └───tpp    数据中台SDK  tppSDK
│        
│    └───utils    工具类目录，一些helper,比如数据连接，sdk调用入口等    
│        
└─── airflow_source    对airflow源码改造的目录，结合docker-compose起作用
└─── plugins
│   .env-example   .env 环境文件的demo 复制修改即可
│   airflow.cfg-example  airflow.cfg demo目录，复制修改即可
│   docker-compose.yaml    docker-compose文件
│   requirements.txt       本地构建时，所需的依赖包
│   webserver_config.py     webserver配置文件
```

### 产品优势
#### 工作流：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 以吃包子而言，买到原材料-->制作包子-->蒸包子-->包子装盘-->吃包子，这就是一个“吃包子流”，即一个吃包子的流程。那么类比的来想，工作流就是指工作的流程。
#### 自动化：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 吃包子这个流程，很多步骤都必须由人工干预。想象一下，未来有一天你只需要按一个按钮，每天都会有准备好的包子从电脑屏幕里蹦到你的办公桌前，这就是自动化，尽可能避免或者完全消除人工工作的部分。
#### 灵活：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 使用Airflow，你可以随心所欲的定义和修改各个任务之间的依赖关系，在失败的情况下，每个任务的重试次数，到底是使用多台电脑还是一台电脑来计算和处理工作流等等。你甚至可以选择不用代码，而是用可视化的视窗工具来修改你的工作流。
#### 可扩展：
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 可扩展主要是指Airflow可以通过接入一些其它工具，达成分布式计算的效果——如果你的工作流太过庞大复杂，可能会需要这个。

#### 新上功能
- 接入了统一登录，登录后具有viewer角色的权限
- 支持多项目，dags目录下可以自由创建项目，项目的dag由项目owner各自创建，各自维护
- 集成了数据中台的SDK,可以注册和调用数据中台的作业
- 创建并集成了tpp的pythonSDK,可以调用统一消息发型邮件等等
- 脚手架功能，支持在本地开发电脑运行docker

### 实战演练
#### 配置计算任务

![](数据中台产品文档.assets\image-20210909152449877.png)

方式一：通过Hive Operator
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 调度中心内置了hive_client和HiveOperator，通过指定conn_id,hql,schema等参数调用

```
HiveOperator(
					hive_cli_conn_id="dashboard_hive",
					task_id="dwd_to_dws",
					schema='hdp_fin_dash_dws',
					hql='hive/hql/dwd_to_dws.sql',
					dag=dag,)
```



方式二：通过SDK
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 中心集成了数据中台计算SDK，要先在数据中台注册JobID，然后再传递JobID和日期参数调用

```
import dags.sdk.middle_platform.run_job as job
job.runJob(JOB_ID, date)
```



#### 配置统一消息发送

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 调度中心创建集成了TPP的统一消息SDK，可以很方便发邮件和企业微信
```
import dags.utils.message as message   # 引入sdk

# task 发邮件示例
PythonOperator(task_id=“extract_new_user”,
		python_callable=extract_new_user,
		retry_delay=timedelta(seconds=30),
		retries=1,
		on_failure_callback=message.task_failure, #任务失败时发送失败邮件             on_success_callback=message.task_failure, #任务成功发送成功邮件
		dag=dag, )
# dag 发邮件示例
dag = DAG(
	dag_id=DAG_ID,
	description="ETL data from superset api",
	default_args=dag_args,
	tags=["Dashboard"],
	schedule_interval="00 15 * * *",
	doc_md=READ_ME,
	on_failure_callback=message.dag_failure, #dag运行失败时发送失败邮件
	on_success_callback=message.dag_success  #dag运行成功时发送失败邮件
	)
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 可以在UI管理后台根据DAG配置相关接收的邮箱地址

![](数据中台产品文档.assets\image-20210909152429577.png)



### 常见问题
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 保留章节，后续补充用；

# 数据应用开发规范

## 引言

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 俗话说无规矩不成方圆，我们基于数据中台底座产品构建数仓时也需要遵循数据中台底座的开发规范进行开发，这样才能少走弯路

## 准备篇

### 注册作业配置参数
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们可以在注册作业时将作业的固定参数注册到作业信息中，后续例行化执行会使用该配置；       
作业配置参数我们可以在注册作业时通过params配置，支持key、value和value两种方式配置，常用的spark和datax配置场景都已满足
```shell
# 注册作业命令
python3 /data/calc/bin/register_conf.py -h

                    Usage: python register_conf.py 配置文件路径 [作业名称]
                    Usage: python register_conf.py [-f 配置文件路径] [-n 作业名称] [-e 配置文件压缩加密字符串] [-t 租户id] [-p 作业运行配置] [-g 执行引擎]
                    Usage: python register_conf.py [--filepath=配置文件路径] [--jobname=作业名称] [--encodedhex=配置文件压缩加密字符串] [--tenant=租户id] [--params=作业运行配置] [--engine=执行引擎]                    
                    
                参数描述    --filepath :配置文件路径 如：./example/example.json
                           --jobname :作业名称 如：job_001
                           --encodedhex :配置文件压缩加密字符串
                           --tenant ：租户id 
                           --params ：运行配置参数，如：{"params":["-Xms1G","-Xmx2G"],"config":{"partition":"20210101"}}
                           --engine : 执行引擎： 0、spark(默认) 1、hive 2、flink 3、flume 4、datax
```

### 作业运行配置
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 例行化执行作业除了需要固定参数配置外，针对不同时间动态切换参数也是我们计算和传输作业所需要支持的    
```shell
# 执行作业命令
python3 /data/calc/bin/run_job.py -h
        Usage: python run_job.py jobId [rundate]
        Usage: python run_job.py -j 'job id' [-d 'run thedate' -x Flase -c '{"channel":"123", "version":"v1001"}']
        Usage: python update_job.py --jobId 'job id' [--thedate 'run thedate' --isdebug False --runconf '{"channel":"123", "version":"v1001"}']
        参数描述：
            -h,--help: 帮助
            -j,--jobId: 作业id
            -d,--thedate: 作业运行日期
            -x,--isdebug: 默认false，作业运行模式，调试 or 正式运行
            -c,--runconf: 作业运行配置 {"channel":"123", "version":"v1001"}
```

* 通过runconf配置的参数，在作业例行化执行时会根据key寻找到${key}，将${key}替换为value
* 通过thedate配置的参数，我们会根据日期特征将参数做加工替换，具体使用规则参考下面表格；
|方法|功能介绍|输入数据|输入时间|输出效果|
|---|---|---|---|---|
|date|根据时间格式将原内容中格式替换为指定单位的时间|select * from table1 where pday=${date(Ymd)}|20210603|select * from table1 where pday=20210603|
|date_add|根据时间格式将原内容中格式替换为指定单位计算后的时间|select * from table1 where pday=${date(Ymd)} or pday=${date_add(Ymd,-30)}|20210603|select * from table1 where pday=20210603 or pday=20210504|
|date_range|根据时间格式将原内容中格式替换指定单位时间范围|select * from table1 where pday in ( ${date_range(Ymd,-3)} )|20210603|select * from table1 where pday in ( '20210601','20210602','20210603' )|

### 数据计算、传输调用
* python utils包形式调用
* sdk命令行方式调用

### 账号要求

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 注册项目账号，通过个人账号跳转至项目账号，由项目账号作为权限管控入口
例如：为xxxx开通sudo权限，通过xxxx sudo切换到hdp-test项目账号

### 存储建设目录格式

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /data/project account/warehouse/db name/table name/partition

### OLAP表建设

#### 创建单节点表

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clikchouse的表有多种类型；MergeTree Family、Log Family、Integrations
、Special等；我们通常使用MergeTree;

```sql
CREATE DATABASE IF NOT EXISTS TEST_DB;

CREATE TABLE IF NOT EXISTS  TEST_DB.TEST_TABLE(
`prj_id` Int64,
 `story_id` Int64,
 `story_name` String,
 `status` String
) ENGINE = MergeTree ORDER BY (prj_id, story_id);
```
#### 创建分布式表

```sql
CREATE TABLE IF NOT EXISTS {distributed_table} as {local_table} 
ENGINE = Distributed({cluster}, '{local_database}', '{local_table}', rand())
```

## 开发篇

### 开发流程

我们采用三套环境
开发   中台调试用
测试   用户测试应用
生产   正式上戏数据应用
上线流程从上到下依次流转

### 表数据列分隔符

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 创建表时列分隔符需选用比较生僻的字符避免和业务数据冲突导致结构映射混乱；
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这里推荐设置未\001

### 动态分区设置

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果使用动态参数，可在注册作业时传入下面配置即可；需要注意的是下面的10000按需修改
```json
{
    "param":[

    ],
    "config":{
        "hive.exec.dynamic.partition":true,
        "hive.exec.dynamic.partition.mode":"nonstrict",
        "mapreduce.map.memory.mb":15000,
        "mapreduce.reduce.memory.mb":15000,
        "hive.merge.mapredfiles":true,
        "hive.exec.max.created.files":100000,
        "hive.exec.max.dynamic.partitions":100000,
        "hive.exec.max.dynamic.partitions.pernode":100000
    }
}
```


### 设置hive表允许执行修改数据
参考
https://blog.csdn.net/sparkexpert/article/details/50432369
https://www.jianshu.com/p/ed6b7c6e7fc3

### 数仓中避免出现空值
通过该方式，数据同步到clickhouse后可提升clickhouse查询效率

## 建立ORC存储表

## orc存储表读写同一张表需要添加下面配置
spark.sql.orc.enableVectorizedReader=false
spark.sql.hive.convertMetastoreOrc=false


## 问题排查
### 日志查询步骤
1、根据日志输出中作业运行获取运行服务器
[2021-12-22 16:02:14,774] {logging_mixin.py:104} INFO - server url: http://xx.xx.xxx.xx:8001/calc/
2、登录对应服务器
3、数据传输日志：/data/calc/logs/datax_log/yyyy-mm-dd(执行日期)/*作业id.log